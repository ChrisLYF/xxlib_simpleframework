后端

基本需求

稳定, 安全, 易扩展, 易管理, 运营成本不高, 能支撑百万人同时在线



基础结构( 不含 游戏, 登录 / 排队, 活动, 比赛, 商城, 仓储, 聊天, 邮件, 查询, 排行, 日志, BOT, 后台...... )

Manage, Loader, Discovery, Navigate, Proxy, DB, DBCommit



物理分布

Manage 是服务器管理后台( 1 份 ), 借助 Loader, 可针对每台机器 进行 service 开关配置

Loader 每台机器开机自启动运行一份, 连接到 Manage
其他服务 受 Loader 控制, 根据 Manage 的配置, 于相应服务器动态开启

Loader 同时也监视 具体 service 实例的运行健康状态

Discovery 为发现服务, 接受各 service 注册并拉已注册 service 清单( 含实例type, id, ip, port 之类 )
这样子 service 就可以在接下来 根据清单来查找自己感兴趣的 service, 并与之建立连接.
连接过程是双向的. 不管谁依赖谁, 都是后启动的去找先启动的 service 建立连接.

Discovery 服务需要先于其他 service 启动( Manage, Loader 除外 )

Navigate 为导航服务, 其结构本身可能是一个金字塔形, 有可能在多台机上开多份, 但共享相同数据.
导航结果决定了 client 会去连哪个 或哪些 proxy

Proxy 为数据代理, 具有 数据转发 / 汇聚 / 协议转换 / 限速 / QOS / 基础安全校验 / 分流抗攻击 / 确保物理连接不断 等功用

DB 为数据中心( 1 份 ), 内存缓存热数据, 向内网提供各种 高效的 无阻塞的 逻辑单点的 数据操作.

DBCommit 为数据同步提交服务( 1 份 ), DB 的所有内存变化, 均及时生成相应的 数据库操作指令, 并放入 Redis 蓄水池
这个服务从蓄水池不断的取出指令, 逐个执行, 以实现高速及时的修改内存 但也能慢慢的将数据库同步的效果.

DB / DBCommit 需要不怕死, 不怕重启, 能动态恢复数据. 这需要 Redis 那边做一定的已执行指令备份设计, 
以及 DB 本身的操作 需要事务记录诸如 "流水号" 之类的东西, 以便异常到来时, 可定位到相应的 数据库操作指令, 继续执行



网络连接对应关系

Manage 与 Loader 一对多
Discovery 与 services 一对多
Navigate 与 Proxy 一对多
Proxy 与 services 多对多



通信流程与需求
 
client 先从 navigate( 可能有多个, udp, 首包即含复杂校验信息 ) 得到要连接哪个 proxy 的信息, 然后去就去连指定 proxy
    
client 发送相应指令到 proxy. proxy 负责转发到相应 service, 并转发处理结果( 应答 / 推送均可 )
对于断线重连的情况, proxy 应该告知 client, 连接到原有 proxy 上, 并与 context 重连.
proxy 上面理论上讲有一段 待发送 & 已发送的 数据包队列, 其生命周期手工配置, 用于断线重连补发
针对补发的需求, 向 client 发送的包, 汇聚到一个队列, 有 "包编号", 以便于在队列中定位

proxy 同时也应该有 上 & 下行 限速, 带宽均分功能, 用以保护有限的总带宽尽量提升每机玩家在线数
对于可并行下发的流量( 例如 视频流 和 游戏指令 ), 理论上讲需要做分 channel 优先级限速( 切片组合, 先确保关键性数据 channel 的包占据流量大头 )

proxy 同时也应该提供优化后的 udp 通信能力( udp with kcp 之类 ), 以显著降低网络延迟

proxy 同时也应该有 多级转发, 多点接入, 动态切换, 以分摊 / 转移流量, 做到防 ddos 的效果
proxy 同时也应该有基础的攻击检测功能, 并且能与相应的防火墙交互沟通, 做到半实时硬件拦截


proxy context 中, 针对每类指令, 应建立 client 到 service 的双向映射, 以便于双向转发.
当前以 生成器 扫描根命名空间 的方式来区分 service 类型, 公共结构都不加命名空间.
生成器 需要生成与上述规则对应的 根命名空间枚举 以及 ServiceTypeId< 包 >.value = 根命名空间下标 设置代码
考虑令某特殊的 Enum 名字用来枚举这些 根命名空间, 以便命名空间可以任意使用
    
对于一种类型有多个 service 的情况, 存在一个 Redirect 的设计, 即首次转发都是发给同类型 service 的 "交换机"
client context, 存在 "某类型 service 发送目标" 的映射. 如果首次发送, 该映射并未建立. 遂先向 "交换机" 发送导航请求
"交换机" 根据某种理由( 负载均衡? 断线重连? 目标服务已死? ), 向 proxy 回复 redirect
proxy 在收到 redirect 之后, 将该类型 service 在 context 中 建立映射, 并继续发包.
这可能导致数据包滞留. 但也要等到映射建立之后才发送后续.

client 发向 proxy 的包, 可以不带任何地址信息. proxy 通过查询 context 来定位到目标 service 的连接并转发
转发时需要附上该 client 的唯一标识, 以便 service 发送数据过来的时候, 定位到 client 的连接并转发
service 在收到包之后, 也需要对 "从哪个 proxy 收到的 client xxx 的数据" 做相应的映射
这个映射可能会发生改变( 当 service 中的 context 与 proxy 中的 context 生命周期不一致时 ), 后续都是往当前 proxy 发送
    
也存在一种 proxy 死掉的可能性( 代码不稳定? 虚拟主机死机 掉线啥的 ), 考虑采取先暂存一段时间, 待目标确认接收成功后 再移除


    
开局流程

玩家位于大厅时, 如果通过匹配, 定位到了目标战斗 service, 则此时需要将玩家 "引" 过去. 这里 proxy 并不针对此类 service 做 "映射",
而是大厅告知 client 它需要连接的 service 的 address 信息, 在 client 发到 proxy 的包( 用于与目标服务器通信 )中, 需要携带该 address 信息.
同时, 大厅也告知目标 service, 要连进来的 client 的 唯一标识. ( 这个过程有可能先于 大厅告知 client 发生, 此举可以顺便探测目标 service 的健康 )

该做法相比同类型映射方案, 可以更灵活的应对需要 "同时进行", "多开" 的需求. 
但相应的, 也有暴露 服务端 逻辑流程的风险, 恶意 client 可能直接伪造目标地址以实施某些因 开发过程中的疏忽造成的漏洞攻击
故 此种做法, 只针对部分需要并行的同类型服务运用




带宽需求分析

以棋牌为例, 此类游戏交互量较小. 平均每用户每秒流量可能不足 1k ( 除开流媒体相关 ). 甚至部分游戏类型间隔几秒才操作一次, 平时基本没啥流量

如果设计层面存在 "进出大厅, 能看到桌子上坐有人" 的概念, 则玩家进出大厅的数据, 需要及时同步,
刚进入大厅时的流量 可能略大, 这和大厅内有多少玩家, 以及每玩家的数据的详尽程度有关. 

如果设计层面存在 "玩家形象展示" 的概念, 则玩家除开本身的 id, 金币, 等级?? 成就?? 这些少量数据以外, 还会附带纸娃娃数据表
如此估算下来, 恶劣情况下, 每玩家数据可能达到 1k 一位. 设 每"大厅" 平均 200 名玩家, 则每次进出大厅, 可能产生 200k 瞬间流量.
如果遇到恶意客户端, 在服务器没有防备的情况下, 模拟频繁进出人多的房间, 则极易耗光带宽
    
配合上文中提到的 proxy 的职能, 如果做限速, 针对这种大厅数据, 又会造成客户端体验受到影响( 比如进出大厅加载时间显著延长 )
这属于不可调和之矛盾, 理论上讲可通过与策划案配合, 以避免出现此类情况. 
比如限制每 "大厅" 的同时在线数, 允许分时延迟加载纸娃娃数据, 客户端做本地玩家数据缓存( 带版本号, 过期时间, 按需批量 / 个体加载 )
控制得好的情况下, 进出大厅时, 只收发每玩家的基础数据( id, 版本号 ), 200 * 12 = 2.4k, 完全可以接受
之后, 客户端根据本地缓存, 分时向 查询服务 拉取玩家具体的展示数据, 1秒拉 2 人? 这样也就能将流量控制在 2k 左右, 也可以接受
多数情况下, 客户端会根据显示区域, 优先拉取部分急需显示的玩家数据, 或是已经进入游戏开打, 此时便不再继续产生流量.

基于上文提到的多 proxy 特性, 理论上讲玩家也可以同时建立对多个 proxy 的连接, 数据可从不同的 proxy 下发, 进一步降低费用
具体的, 将 proxy 按带宽特性分组, 大体分为 "固定带宽" 与 "峰值 / 按流量计费" 两种.
几秒一次的稳定 & 少量的游戏内通信, 可归并到 "固定带宽", 而进出大厅 拉排名 之类的行为, 可归并到 "峰值 / 按流量计费"
这样一方面大流量数据不需要限速太多( 还是要限 ), 有助于改善玩家体验, 另一方面也能节省网络费用
    



硬件平台, 指标 与开发分析

考虑到省钱, 高效, 上述核心服务应该使用 C++ 语言开发, 并运行于 linux 服务器平台. 非核心服务, 是否 C++ 开发, 并不关注

// todo: 进一步阐述每种服务的典形压力与 cpu 内存 网络 磁盘 占比











前端功能需求简单分析

棋牌游戏最大的特点, 就是资源单调, 多游戏间复用率高. 
程序方面, 主要考虑的是 "包体小, 热更方便".
包体分两部分构成: 引擎固有占用, 游戏资源( 代码文件也算作资源 )

经调研, u3d 引擎固有占用 随着其版本推进, 越来越大. 截止最新 2017 版启用 .net 4.5 支持来看, 空包已达到了 45M (最小集)
unreal 4 引擎固有占用据官方公布的数据, 空包也达到了 4xM (最小集)
cocos2d-x 2.x 版本在一般情况下, 空包可以搞到 2-3M, 手工精简后甚至能到 1M. 但是缺 3d 功能, 编码老旧, 缺陷较多
cocos2d-x 3.x 版本在不关闭任何功能的情况下, 空包约为 9M. 手工精简后, 有望达到 6M( 含基础 3d 功能, 至少能做 QQ欢乐麻将 那样的效果 )

棋牌游戏 3d 相较于 2d 来说, 理论上讲能做到更小的包体及显示质感. 对于开发量来讲, 并不会显著提升.
包小缩小的原因是 3d 贴图复用, uv / 顶点动画.  2d 如果考虑卖皮换肤, 特效好看之类, 就会产生巨量的图片. 且这些图片在高清设备上需要的显示精度更高, 实际包体可能会比较巨大.

一个棋牌典型的包体积计算, 以麻将 / 设计精度为 2048 * 1536 为例( 非压缩 ): 
UI图元 16M (公有) + 人物 / 成就展示 32M (公有) + 行为特效( 胡牌啥的 ) 32M + 麻将所有牌面拼合 16M * N套皮肤

基于 PVRTC4 等贴图压缩格式, 可以将上述字节数, 降低至 1/4 ( 并非所有贴图都能如此压缩降低. 有些部分压缩后质量可能令人无法接受 )
基于 spine 等骨骼动画功能, 改善 "特效" 部分, 理论上讲 能降至 1/4 左右
基于 3d 贴图, 改善 麻将牌面 及 皮肤部分, 理论上讲, 能降至 1/2 左右. 部分皮肤还可以走更换模型的路子, 不吃贴图.
计算结果为: UI 4M + 人物 8M + 特效2M  + 麻将 2M * N 套皮肤

进一步的, 如果打包时只考虑最 "必要" 的元素, 其他东西 "运行时动态下载", 则 "人物" 部分 以及  麻将额外皮肤 都可以不必计入初始包,
计算结果为 : UI 4M + 基础人物 1M + 特效2M  + 麻将 2M * 1 ~= 9M

再加上代码, 基础音乐音效等( 4M ), 理论上讲能控制到 13M 以内( 音效其实也存在做细的可能, 比如采样率, 本地口音, 明星配音等, 所以有可能比这里估计的要大, 但可以动态下载)

结合 engine 的固有占用, 包体约为 13M + 6M = 19M ( cocos 3.x )  或 13M + 45M = 58M ( u3d / ue4 )
如果全面为包体优化, 使用 "全下载模式", 即发行包只含有 splash 以及 downloader, 那理论上讲是能做到 10M 以内的( cocos ).


动态下载方面, 结合所谓 "百万在线" 的需求, 可以比较容易的得到一个事实, 即 自己直接架资源服务器, hold 不住更新时的并发下载量.
基于业界成熟的 CDN 方案, 可解决并发流量问题. 
内网需要开发一套 基于发布版本控制的 "资源差异打包自动上传系统"
例如: 上传至 七牛等 CDN 空间并使用相应 API 令其生效, 最后通知管理员 文件已上传, 可以做版本切换( 版本切换 是一套复合逻辑, 需要服务器间配合, 这里不做详细阐述 )


结合热更, 客户端上需要有一套简易的 "文件管理系统". 在进入 "需要下载或更新"的功能时, 或收到服务器通知后, 可到指定 URL CDN 拉取
文件管理系统 主要负责记录每个文件的 要素, 以及额外配置
其字段包括 文件名, 类型, 版本号, 尺寸, 目录路径, 位于哪个补丁包内, 下载校验信息, 逻辑功能依赖, 目标显示尺寸( 2d 需要 ), 是否关闭抗锯齿 等等
运行时结合这样一套数据库结构, 通过计算来选择性下载解包.
考虑到中国网络问题, 下载时需要做到类似 迅雷那样的 "多线" 同时下载


通讯方面, 需要与后端配合, 直接收发 2进制 数据, 从逻辑上要能同时连接多个 proxy ( 协议可能是 udp 或 tcp ), 做包排序.
后端网络包设计器当前支持 "继承", "递归指针" 等特性, 客户端理论上讲也需要配套模拟实现与之相应的功能.


动态内容方面, "富文本" 内容的显示支持, 是需要的. 一般的图文混排, 缩进, 自动换行, 改变字体字号颜色, 基本功能需要有.
主用于制作 聊天窗体, 公告, 活动介绍, 帮助文字


多点触摸方面, 分层防击穿, 局部单路touch限制, 事件逻辑阻断, 误操作 / 无效操作探测 过滤, 这些要有

显示管理方面, 贴图缓存, 填充率优化网格, 引用计数, 运行时异步加载 / 卸载, 这些要有
控件方面, 多分辨率 自适应, 切割悬靠逻辑要有, 浏览器显示插件要有
新手引导的注入 / 限制式操作层面的设计要有, 逻辑窗体间的跳转导航的设计要有
手工动画 / 特效播放控制要有方式实现( 主要针对游戏内局部显示暂停或变速的情况 )









工期 / 时间 / 里程杯 / 人员需求

// todo: 这个要等具体的案子

